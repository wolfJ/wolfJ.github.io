<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-index" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/12/index/" class="article-date">
  <time datetime="2018-03-12T08:17:05.000Z" itemprop="datePublished">2018-03-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/12/index/">Welcome to here.</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>hello, welcome to wolfJ`s Blog.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/12/index/" data-id="cjenyeyey0000sjyg99ljcd32" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-lvs_cluster_solution" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/12/lvs_cluster_solution/" class="article-date">
  <time datetime="2018-03-12T08:02:39.000Z" itemprop="datePublished">2018-03-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/12/lvs_cluster_solution/">Linux服务器集群方案介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>服务(器)集群，涉及的知识点相对偏OS底层些，所以想理解掌握整体的方案，需要把相关的知识点系统的掌握。<br>centos7+nginx+keepalived实践，及原理分析<br>centos7+lvs+keepalived实践，及原理分析<br>本文尽可能的关注解决方案的整体与关键点，弱化介绍方案中各软件应用的参数配置.</p>
<p>##依赖知识点<br>大致可分为三块：<br>OS(Centos7，不同系统、版本间会有细微差异)，<br>网络(协议，拓扑，流向)，<br>应用(上层应用，如tomcat,mysql,等).</p>
<h2 id="概念名词"><a href="#概念名词" class="headerlink" title="概念名词"></a>概念名词</h2><p>Linux, Cluster, Load Balancer(LB), High Availability(HA), High Performance(HPC), Node, DNS, FailOver, VRRP<br>Director,RealServer, Proxy<br>DR,NAT,TUN,<br>Nginx, Keepalived, LVS, F5, IPVS, KTCPVS, NetFilter<br>集群，负载，高可用，负载均衡器，节点<br>扩展性，<br>二层、三层、四层交换机、路由器，反向代理，中间人<br>四层交换，七层交换，<br>Netfilter框架</p>
<h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><p>安装配置、启动<br>服务器要求/性能？<br>方案选型？</p>
<p>方案一：先介绍服务器形态，再介绍采用四层/七层交换，完成负载，介绍各种形态的方法，优劣。<br>方案二：由简单到复杂，先七层，后三层，先独立，后混合。</p>
<h2 id="场景介绍"><a href="#场景介绍" class="headerlink" title="场景介绍"></a>场景介绍</h2><h3 id="简单经典场景"><a href="#简单经典场景" class="headerlink" title="简单经典场景"></a>简单经典场景</h3><h4 id="单台负载调度器-多台真实服务器-负载"><a href="#单台负载调度器-多台真实服务器-负载" class="headerlink" title="单台负载调度器 + 多台真实服务器; 负载"></a>单台负载调度器 + 多台真实服务器; 负载</h4><p>nginx实现负载，老版本支持七层负载，新版本也支持四层负载了。<br><strong>参考连接：</strong><br><a href="https://www.jianshu.com/p/90831a94ce43" target="_blank" rel="noopener">nginx负载均衡配置</a><br><a href="https://wuguiyunwei.com/index.php/2017/06/14/919.html" target="_blank" rel="noopener">nginx四层负载均衡配置代理Mysql集群</a></p>
<h4 id="双台负载调度器-多台真实服务器-主备"><a href="#双台负载调度器-多台真实服务器-主备" class="headerlink" title="双台负载调度器 + 多台真实服务器; 主备"></a>双台负载调度器 + 多台真实服务器; 主备</h4><p>nginx实现负载，keepalived实现HA。<br><strong>参考连接：</strong><br><a href="http://seanlook.com/2015/05/18/nginx-keepalived-ha/index.html" target="_blank" rel="noopener">Nginx+Keepalived实现站点高可用</a></p>
<p>Nginx四层负载，工作在OS的用户空间，性能略差，但具体怎么样要测试下，还看到了付费？仔细了解一下。网上大多是以前的文章了，工作在七层。</p>
<h3 id="LVS负载场景"><a href="#LVS负载场景" class="headerlink" title="LVS负载场景"></a>LVS负载场景</h3><h4 id="单台负载调度器-多台真实服务器-负载-1"><a href="#单台负载调度器-多台真实服务器-负载-1" class="headerlink" title="单台负载调度器 + 多台真实服务器; 负载"></a>单台负载调度器 + 多台真实服务器; 负载</h4><p><strong>参考连接：</strong><br><a href="https://www.jianshu.com/p/2ed85a5204cc" target="_blank" rel="noopener">LVS(NAT和DR)模式详细与配置</a><br><a href="http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/LVS-HOWTO.LVS-DR.html" target="_blank" rel="noopener">LVS: LVS-DR</a><br><a href="http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/LVS-HOWTO.LVS-NAT.html" target="_blank" rel="noopener">LVS: LVS-NAT</a>  </p>
<h4 id="双台负载调度器-多台真实服务器-主备-1"><a href="#双台负载调度器-多台真实服务器-主备-1" class="headerlink" title="双台负载调度器 + 多台真实服务器; 主备"></a>双台负载调度器 + 多台真实服务器; 主备</h4><p>通常又分两种模式，NAT模式，DR模式<br><strong>参考连接：</strong><br>1.LVS + Keepalived NAT模式<br><a href="https://docs.oracle.com/cd/E52668_01/E54669/html/section_xsx_wl2_4r.html" target="_blank" rel="noopener">17.7 Configuring Load Balancing Using Keepalived in NAT Mode</a></p>
<p>2.LVS + Keepalived DR模式<br><a href="https://docs.oracle.com/cd/E52668_01/E54669/html/section_wkd_ys2_4r.html" target="_blank" rel="noopener">17.8 Configuring Load Balancing Using Keepalived in DR Mode</a>  </p>
<h4 id="LocalNode-单负载-多真实服务器"><a href="#LocalNode-单负载-多真实服务器" class="headerlink" title="LocalNode+单负载+多真实服务器"></a>LocalNode+单负载+多真实服务器</h4><p>lvs 单负载调度器，集成在某台真实服务器，实现负载.<br>1.1 NAT模式<br>未尝试</p>
<p>1.2 DR模式<br>未尝试  </p>
<h4 id="LocalNode-两台负载负载-多真实服务器"><a href="#LocalNode-两台负载负载-多真实服务器" class="headerlink" title="LocalNode+两台负载负载+多真实服务器"></a>LocalNode+两台负载负载+多真实服务器</h4><p>lvs + keepalived 两台负载调度器，集成在某两台真实服务器中，实现负载及高可用。<br>2.1 NAT模式<br>未尝试</p>
<p>2.2 DR模式<br><strong>参考连接：</strong><br><a href="http://kb.linuxvirtualserver.org/wiki/Building_Two-Node_Directors/Real_Servers_using_LVS_and_Keepalived" target="_blank" rel="noopener">Building Two-Node Directors/Real Servers using LVS and Keepalived</a><br><a href="http://linbo.github.io/2017/08/20/lvs-dr" target="_blank" rel="noopener">LVS DR模式的一些问题</a><br><a href="http://gcharriere.com/blog/?p=339" target="_blank" rel="noopener">Failover and loadbalancer using keepalived (LVS) on two machines -ok</a></p>
<h3 id="场景图说明"><a href="#场景图说明" class="headerlink" title="场景图说明"></a>场景图说明</h3><p>lvs + keepalived NAT mode:<br><img src="../images/nat.png" alt="lvs + keepalived NAT模式"></p>
<p>lvs + keepalived DR mode:<br><img src="../images/dr.png" alt="lvs + keepalived DR模式"></p>
<h2 id="场景详解"><a href="#场景详解" class="headerlink" title="场景详解"></a>场景详解</h2><p>参见每个具体的场景介绍</p>
<p>独立部署步骤</p>
<ul>
<li>Director节点安装ipvsadm，承担LVS的负载调度能力  </li>
<li>Director节点安装keepalived，负责VRRP的配置，对故障处理(Failover)实现LVS的高可用</li>
<li>在Director上配置keepalived，先后配置主/备、权重、IP负载均衡技术、调度算法、监听IP，负载IP等信息、、、、</li>
<li>RealServer配置VIP，用于接收目标是VIP的网络包</li>
<li>RealServer部署自己的服务应用(apache,nginx,tomcat…)</li>
<li>防火墙开启相应端口</li>
</ul>
<p>All machines in the LVS have the VIP: only the VIP on the director replies to arp requests, the VIP on the realservers must be on a non-arping device (eg lo:0, dummy).</p>
<p>详细配置参见<a href="">lvs+keepalived DR</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>发现好些资料都指向了<a href="http://archive.linuxvirtualserver.org/" target="_blank" rel="noopener">archive.linuxvirtualserver.org</a>，但这个网站显示服务器意外中断，以https访问时也显示页面不存在。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/12/lvs_cluster_solution/" data-id="cjenybl0z0001qmygrhphpcrr" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-lvs_cluster_localnode/index" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/04/lvs_cluster_localnode/index/" class="article-date">
  <time datetime="2018-03-04T09:46:00.000Z" itemprop="datePublished">2018-03-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/load-balance/">load balance</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/04/lvs_cluster_localnode/index/">LVS：LocalNode / Two Box LVS / LVS集成模式配置与原理详解</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>##LVS：LocalNode<br>LVS经典部署场景是Director服务器与RealServer服务器分开部署，独立服务器，一来是为了充分利用硬件性能，二来是为了避免故障相互影响。<br>然而某些时候会把Director与RealServer视为一个软件模块部署在同一台服务器上，用两台服务器部署两套软件，实现负载与高可用。  </p>
<p>###Two Box LVS<br>仅在两台设备上同样可以完整的实现LVS的故障处理。其中一台设备承担了Director的角色，同时也承担了RealServer的角色。另一台设备是真实服务器。两台设备运行故障处理的代码，允许切换另一台设备为Director角色。两台设备是即实现Director，又有RealServer故障保护功能的最少设备方案了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">It&apos;s possible to have a fully failover LVS with just two boxes. The machine which is acting as director, also is acting as a realserver using localnode. The second box is a normal realserver. The two boxes run failover code to allow them to swap roles as directors. The two box machine is the minimal setup for an LVS with both director and realserver functions protected by failover.</span><br></pre></td></tr></table></figure></p>
<p>注意：此方案是在内核版本2.6之后，之前也有相应的补丁，具体参见<a href="">LVS-HOWTO</a>文章。</p>
<p>###架构图</p>
<p><img src="./two-box-lvs-architecture.png" alt="two-box-lvs-architecture"></p>
<p>####集成部署的问题</p>
<p>如果集成部署时，仍按照独立方案的配置来，会存在50%的问题  </p>
<ul>
<li>client 发请求包给 master director</li>
<li>50% 机会 master 把包转给 backup （因为 backup 同时也是 RealServer）</li>
<li>因为 backup 的 LVS rules 已经启用，所以50%机会 backup 把包转给 master</li>
<li>master 收到包后，又可能把包转给 backup，然后陷入死循环。 </li>
</ul>
<p>###解决问题实现思路<br>在两台机器上实现LVS有以下几种思路可以实现：  </p>
<ul>
<li><p>方案一：对网络包打Mark<br>通过iptables对收到的网络包(packets)打Mark。  </p>
</li>
<li><p>方案二：更改网络包流向<br>通过设置netfilter规则，改变网络包的流向。  </p>
</li>
<li><p>方案三：sysctl flag backup_only<br>有人提到在 kernel 3.9+ 上，新增了一个 flag: backup_only=1，解决这个问题。但未尝试，不太清楚如果主备切换，flag 怎么做。</p>
</li>
</ul>
<p>###网络包流向介绍<br>在此之前先介绍一下网络数据包进入操作系统后的数据流向，了解了内核原理，就能很自然的明白两种思路及其实现了。</p>
<p>Netfilter-packet-flow 数据包流向<br><img src="./Netfilter-packet-flow.jpg" alt="packet-flow-in-netfilter"></p>
<p>Netfilter-packet-flow 数据包流向(详细)<br><img src="./Netfilter-rule-flow.png" alt="packet-flow-in-netfilter(详细)"></p>
<!-- 111 -->
<p>网络数据包在LVS Director设备中的流向</p>
<ol>
<li>某块网卡(硬件/Device)接收到数据包(packets)</li>
<li>进入netfilter的INPUT规则链</li>
<li>再进入系统的内核模块IPVS</li>
<li>根据负载技术和调度算法，更改包的目标地址(to RealServer)</li>
<li>出netfilter的OUTPUT规则链</li>
</ol>
<p>###一方案：对网络包标记Mark</p>
<p>####方案思路<br>两台设备相同的架构，当设备接收到请求包，在入口处做标识，区分出是正常请求，还是来自于Director的转发请求。如果是正常请求，则进入转发模块，根据调度策略，转给本设备或另一台设备处理；如果是转发请求，则不需要再进入本设备转发模块了，直接交由RealServer处理。</p>
<p>####标记Mark的原理图<br><img src="./introduce-lvs-mark.png" alt="打Mark的原理图">  </p>
<p>客户端向VIP发出请求或者另设备负载分流请求时，本设备接收到请求，经过防火墙的INPUT过滤模块，mark标记规则：不是来自于另一设备的分流请求就标记mark=3，从另一设备网卡的MAC地址出来的分流请求则不作标记。<br>&emsp;&emsp;监控在enp0s3上负载模块，监控条件也改为mark判断，满足mark=3的包，才进入负载模块，不满足的就跳过负载模块。<br>&emsp;&emsp;进入负载模块的包，会受调度算法，其它参数配置等影响，可能转向本地的IP地址和端口，从而被本设备的RealServer处理，也可能转向另一设备的IP地址和端口，进而跳出本设备。<br>&emsp;&emsp;不进入负载模块的包，因为请求包的目标是VIP，lo上也配置了VIP地址，所以网络包经过了enp0s3后还会被lo的网卡接收到，从而进入用户空间的RealServer模块，处理完后转出本设备。</p>
<p>####详细配置</p>
<p>####集成部署配置步骤</p>
<ol>
<li>两台设备上安装基础软件</li>
<li>两台设备上设置Mark规则</li>
<li>两台设备上配置keepalived </li>
<li>两台设备上配置VIP/VRRP规则</li>
</ol>
<!--
先后配置主/备、权重、IP负载均衡技术、调度算法、虚拟服务，负载IP等信息、、、、
用于接收目标是VIP的网络包，交由服务应用处理
-->
<p>#####安装基础软件</p>
<p>Centos7.3-minimal，通过epel源安装基础软件: </p>
<pre><code>wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm   
rpm -ivh epel-release-latest-7.noarch.rpm   
yum install -y keepalived  
yum install -y ipvsadm  
yum install -y nginx  
</code></pre><p>规划IP地址</p>
<pre><code>#设备A
RIP: 172.16.0.31  interface: enp0s3
VIP: 172.16.0.30
#设备B
RIP: 172.16.0.32  interface: enp0s3
VIP: 172.16.0.30    
</code></pre><p>启用服务/开机启动/防火墙设置/SELinux关闭:  </p>
<pre><code>touch /etc/sysconfig/ipvsadm 
systemctl enable ipvsadm
systemctl start ipvsadm

systemctl enable keepalived
systemctl start keepalived

systemctl enable nginx
systemctl start nginx

firewall-cmd --add-port=80/tcp --permanent
firewall-cmd --reload
firewall-cmd --list-ports

# SELinux disable
setenforce 0
getenforce
# 永久生效: vi /etc/selinux/config
SELINUX=disabled
</code></pre><p>区分RealServer的内容</p>
<pre><code>#设备A 更新nginx首页
echo &apos;nginx a&apos; &gt; /usr/share/nginx/html/index.html
#设备B 更新nginx首页
echo &apos;nginx b&apos; &gt; /usr/share/nginx/html/index.html    
</code></pre><p>#####设置Mark规则<br>Director A 配置 iptables ，除了 Director B 以外的包，都标记 mark 为3</p>
<pre><code># iptables  -t mangle -I PREROUTING -d $VIP -p tcp -m tcp --dport $VPORT -m mac ! --mac-source $MAC_Director_B -j MARK --set-mark 0x3 
iptables  -t mangle -I PREROUTING -d 172.16.0.30 -p tcp -m tcp --dport 80 -m mac ! --mac-source 08:00:27:E1:3C:C5 -j MARK --set-mark 0x3
</code></pre><p>Director B 配置 iptables ，除了 Director A 以外的包，都标记 mark 为4</p>
<pre><code># iptables  -t mangle -I PREROUTING -d $VIP -p tcp -m tcp --dport $VPORT -m mac ! --mac-source $MAC_Director_A -j MARK --set-mark 0x4
iptables  -t mangle -I PREROUTING -d 172.16.0.30 -p tcp -m tcp --dport 80 -m mac ! --mac-source 08:00:27:77:84:EF -j MARK --set-mark 0x4
</code></pre><p> 查看已经设置的规则</p>
<pre><code>iptables -vL -t mangle
</code></pre><p>#####配置keepalived<br>设备A(Master)配置如下：<br><code>vi /etc/keepalived/keepalived.conf</code></p>
<pre><code>global_defs {
   router_id LVS_DEVEL
}
vrrp_instance VI_1 {
    state MASTER
    interface enp0s3
    virtual_router_id 51
    priority 100
    advert_int 1
       authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        172.16.0.30
    }
}
virtual_server fwmark 3 {
    delay_loop 30
    lb_algo rr
    lb_kind DR
    persistence_timeout 0
    protocol TCP

    real_server 172.16.0.31 80 {
        TCP_CHECK {
               connect_timeout 3
        }
    }
    real_server 172.16.0.32 80 {
        TCP_CHECK {
               connect_timeout 3
        }
    }
}
</code></pre><p>设备B(Backup)与设备A(Master)基本一致，差异如下：  </p>
<pre><code>vrrp_instance VI_1 {
    state BACKUP
    priority 50
    ...
}

virtual_server fwmark 4 {
    ...
}
</code></pre><p>#####配置VIP/VRRP规则<br>enp0s3 上的VIP已经由keepalived模块配置好了，不需要我们多做设置，只需要将 lo 上的VIP配置好即可。<br>两台设备上都创建此文件 <code>vi /etc/sysconfig/network-scripts/ifcfg-lo:0</code></p>
<pre><code>DEVICE=lo:0
IPADDR=172.16.0.30
NETMASK=255.255.255.255
ONBOOT=yes
NAME=loopback
</code></pre><p>修改文件 <code>vi /etc/sysctl.conf</code> 添加以下内容 </p>
<pre><code># 配有VIP的网卡enp0s3忽略arp包，让lo处理
net.ipv4.conf.enp0s3.arp_ignore = 1 
net.ipv4.conf.enp0s3.arp_announce = 2 
# 允许包转发 
net.ipv4.ip_forward = 1
</code></pre><p>使修改生效  </p>
<pre><code>ifup lo 
sysctl -p
</code></pre><p>防火墙设置VRRP接收规则，允许两台设置的keepalived之间通信，实现主/备切换</p>
<pre><code># 指定keepalived配置的网卡：enp0s3，固定的VRRP广播地址：224.0.0.18
firewall-cmd --direct --permanent --add-rule ipv4 filter INPUT 0 --in-interface enp0s3 --destination 224.0.0.18 --protocol vrrp -j ACCEPT
firewall-cmd --direct --permanent --add-rule ipv4 filter OUTPUT 0 --out-interface enp0s3 --destination 224.0.0.18 --protocol vrrp -j ACCEPT
firewall-cmd --reload
</code></pre><p>###二方案：设置Netfilter规则  </p>
<p>####方案思路<br>50%的问题是因为网络包进入了两台设备的Director模块。那能不能让网络包不进入两台设备的Director模块，只进某一台设备Director模块呢？Master角色的这台设备肯定会承担所有的网络包入口，Backup角色的这台设备只承担了Master分流过来的网络包请求，Backup的设备在收到网络包，在转入Director模块之前，添加一个规则，让网络包跳过Director模块，直接转到用户空间的RealServer模块处理，这样就不会有50%的问题存在了。</p>
<p>####方案原理图</p>
<p>Master角色设备的包流向原理图：<br><img src="./introduce-lvs-netfilter-1.png" alt="netfilter原理图">   </p>
<p>Backup角色设备的包流向原理图：<br><img src="./introduce-lvs-netfilter-2.png" alt="netfilter原理图">  </p>
<p>Keepalived的脚本负责维护netfilter中INPUT的规则，根据设备的角色，执行相应的脚本来设置规则或是清理规则。<br>当角色为Master时，清除掉规则，所有请求VIP的包，都正常的被LVS的内核模块监听接收处理，按算法或参数，交由本设备的RealServer处理，或是转至另一台设备处理。<br>当角色为Backup时，添加规则，所有请求VIP的包，将被强制Redirect至本设备的RealServer处理，处理完后，再转出本设备。<br>我们会发现这种方法更加简单，高效些。</p>
<p>####集成部署设置步骤</p>
<ol>
<li>两台设备上安装基础软件  </li>
<li>两台设备上准备脚本  </li>
<li>两台设备上配置keepalived  </li>
<li>两台设备上配置VIP/VRRP规则  </li>
</ol>
<p>####详细配置  </p>
<p>#####基础安装软件<br>与方案一相同.  </p>
<p>#####准备脚本<br>下载<a href="./bypass_ipvs.sh">bypass_ipvs.sh</a>脚本文件，放于目录<code>/etc/keepalived/</code>中。</p>
<pre><code># 加执行权限
chmod +x /etc/keepalived/bypass_ipvs.sh
</code></pre><p>#####配置keepalived<br>设备A (Master)的配置<br><code>vi /etc/keepalived/keepalived.conf</code></p>
<pre><code>global_defs {
   router_id LVS_DEVEL
}
vrrp_instance VI_1 {
    state MASTER
    interface enp0s3
    virtual_router_id 51
    priority 100
    advert_int 1
       authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        172.16.0.30/24
    }
   # Invoked to master transition
   notify_master &quot;/etc/keepalived/bypass_ipvs.sh del 172.16.0.30&quot;
   # Invoked to slave transition
   notify_backup &quot;/etc/keepalived/bypass_ipvs.sh add 172.16.0.30&quot;
   # Invoked to fault transition
   notify_fault &quot;/etc/keepalived/bypass_ipvs.sh add 172.16.0.30&quot;
}
virtual_server 172.16.0.30 80 {
    delay_loop 30
    lb_algo rr
    lb_kind DR
    persistence_timeout 0
    protocol TCP

    real_server 172.16.0.31 80 {
        TCP_CHECK {
            connect_timeout 3
        }
    }
    real_server 172.16.0.32 80 {
        TCP_CHECK {
            connect_timeout 3
        }
    }
}
</code></pre><p>设备B (Backup)与设备A (Master)基本一致，差异如下：  </p>
<pre><code>vrrp_instance VI_1 {
    state BACKUP
    priority 50
    ...
}
</code></pre><p>#####配置VIP / VRRP规则<br>enp0s3 上的VIP已经由keepalived模块配置好了，不需要我们多做设置。也不需要设置lo的VIP。</p>
<p>修改文件 <code>vi /etc/sysctl.conf</code> 添加以下内容 </p>
<pre><code># 允许包转发 
net.ipv4.ip_forward = 1
</code></pre><p>使生效</p>
<pre><code>sysctl -p
</code></pre><p>防火墙设置VRRP接收规则，允许两台设置的keepalived之间通信，实现主/备切换</p>
<pre><code># 指定keepalived配置的网卡：enp0s3，固定的VRRP广播地址：224.0.0.18
firewall-cmd --direct --permanent --add-rule ipv4 filter INPUT 0 --in-interface enp0s3 --destination 224.0.0.18 --protocol vrrp -j ACCEPT
firewall-cmd --direct --permanent --add-rule ipv4 filter OUTPUT 0 --out-interface enp0s3 --destination 224.0.0.18 --protocol vrrp -j ACCEPT
firewall-cmd --reload
</code></pre><p>####补充说明<br>享受成功的喜悦吧：）</p>
<p>多亏了nat规则，我们成功的在两台机器上搭建了负载匀衡和自动故障处理配置。采用这种架构可以完全的最高效的使用可使用的资源。一台设备承担了负载和RealServer角色，当发现这台设备故障时，另一台可以快速切换为负载和RealServer角色。备份设备不仅检查Master设备的状态，同样也处理从负载分发过来的请求。</p>
<p>Keep in mind that the request distribution is not strictly made on a connection basis. The clients connect to one of the realserver going through the VIP. Once this is done and as soon as the realserver in question is available,  further requests will be forwarded to the same realserver. In a classical scenario, many clients connect to the VIP, thus the global amount of requests is equally distributed between the two nodes.</p>
<p>###调试排错<br>如有需要，随后附上调试排错的方法，和详细步骤。<br>例如：<br>在测试过程中，客户端一直没有响应？<br>或者为什么请求一直不进入负载模块，等。。。？  </p>
<p>###参考<br><a href="http://kb.linuxvirtualserver.org/wiki/Building_Two-Node_Directors/Real_Servers_using_LVS_and_Keepalived" target="_blank" rel="noopener">Building Two-Node Directors/Real Servers using LVS and Keepalived</a>  </p>
<p><a href="http://linbo.github.io/2017/08/20/lvs-dr" target="_blank" rel="noopener">LVS DR模式的一些问题</a>  </p>
<p><a href="http://gcharriere.com/blog/?p=339" target="_blank" rel="noopener">Failover and loadbalancer using keepalived (LVS) on two machines -ok</a>  </p>
<p><a href="https://www.jianshu.com/p/734640384fda" target="_blank" rel="noopener">Linux内核参数之arp_ignore和arp_announce</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/04/lvs_cluster_localnode/index/" data-id="cjenybl1s0002qmygfb99xd3u" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/keepalived/">keepalived</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/localnode/">localnode</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/lvs/">lvs</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/two-box/">two box</a></li></ul>

    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Kategorien</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/load-balance/">load balance</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/keepalived/">keepalived</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/localnode/">localnode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/lvs/">lvs</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/two-box/">two box</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/keepalived/" style="font-size: 10px;">keepalived</a> <a href="/tags/localnode/" style="font-size: 10px;">localnode</a> <a href="/tags/lvs/" style="font-size: 10px;">lvs</a> <a href="/tags/two-box/" style="font-size: 10px;">two box</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/03/12/index/">Welcome to here.</a>
          </li>
        
          <li>
            <a href="/2018/03/12/lvs_cluster_solution/">Linux服务器集群方案介绍</a>
          </li>
        
          <li>
            <a href="/2018/03/04/lvs_cluster_localnode/index/">LVS：LocalNode / Two Box LVS / LVS集成模式配置与原理详解</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>